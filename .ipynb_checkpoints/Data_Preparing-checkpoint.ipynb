{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate the qapair.txt \n",
    "# File qapair.txt is in this format : \n",
    "# Question_id , Answer0_vote , Answer1_vote , ... , NumOfAnswer\n",
    "def generate_qapair(PATH_IN, PATH_OUT):\n",
    "    \n",
    "    # f_in  = open(PATH_IN + 'answer.json', 'r')\n",
    "    # f_out = open(PATH_OUT + 'qapair.txt', 'w') \n",
    "    \n",
    "    with open(PATH_IN + 'answer.json' , 'r') as f_in, \\\n",
    "         open(PATH_OUT + 'qapair.txt' , 'w') as f_out :\n",
    "            \n",
    "            line1 = f_in.readline()\n",
    "            info1 = line1.split('\"')\n",
    "            pre_qid = info1[3]\n",
    "            pre_vote = info1[-2]\n",
    "            f_out.write(pre_qid+','+pre_vote)\n",
    "\n",
    "            num_vote = 0\n",
    "            for line in f_in:\n",
    "                info = line.split('\"')\n",
    "                cur_qid = info[3]\n",
    "                # print cur_qid\n",
    "                cur_vote = info[-2]\n",
    "                if cur_qid == pre_qid :\n",
    "                    f_out.write(','+cur_vote)\n",
    "                    num_vote = num_vote + 1\n",
    "                else :\n",
    "                    f_out.write(','+str(num_vote + 1))\n",
    "                    num_vote = 0\n",
    "                    f_out.write('\\n'+cur_qid+','+cur_vote)\n",
    "                pre_qid = cur_qid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract numOfAnswer >= 3 qid write to qid_3more\n",
    "# extract numOfAnswer < 3 qid write to qid_3less\n",
    "def extract_qid(PATH_IN, PATH_OUT) :\n",
    "\n",
    "    #f_in = open(PATH_IN + 'qapair.txt', 'r')\n",
    "    #f_out = open(PATH_OUT + 'qid_ex.txt', 'w')    \n",
    "    \n",
    "    with open(PATH_IN + 'qapair.txt', 'r') as f_in, \\\n",
    "    open(PATH_OUT + 'qid_3more.txt', 'w') as f_out1,     \\\n",
    "    open(PATH_OUT + 'qid_3less.txt', 'w') as f_out2:\n",
    "        examples = 0\n",
    "        for line in f_in:\n",
    "\n",
    "            info = line.split(',')\n",
    "            qid = info[0]\n",
    "            num_answer = info[-1]\n",
    "\n",
    "            if int(num_answer) >= 3:\n",
    "                # print num_a\n",
    "                # examples = examples + 1   \n",
    "                best_answer_votes = int( info[1] )\n",
    "                second_answer_votes = int( info[2] )\n",
    "                \n",
    "                # drop the answers which answer_votes == 0  \n",
    "                if best_answer_votes > 0 and best_answer_votes > second_answer_votes : \n",
    "                    f_out1.write(qid+'\\n')\n",
    "                    # print qid`\n",
    "            else :\n",
    "                f_out2.write(qid+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split question.json into \n",
    "# question_train.json (numOfAnswer >= 3 )\n",
    "# question_test.sjon (numOfAnswer < 3 )\n",
    "def split_qustion(PATH_IN, PATH_OUT) :\n",
    "\n",
    "    with open(PATH_IN + 'qid_3more.txt', 'r') as f,         \\\n",
    "        open(PATH_IN + 'question.json', 'r') as f_in,      \\\n",
    "        open(PATH_IN + 'question_train.json', 'w') as f_out1, \\\n",
    "        open(PATH_IN + 'question_test.json', 'w') as f_out2: \n",
    "        qid_list = [line.strip() for line in f]\n",
    "\n",
    "        # f_in = open(PATH_IN + 'question.json', 'r')\n",
    "        # f_out = open(PATH_OUT + 'question_filtered.json', 'w')\n",
    "\n",
    "        for line in f_in:\n",
    "            info = line.split('\"')\n",
    "            qid = info[-2]\n",
    "            if qid in qid_list:\n",
    "                # print qid\n",
    "                f_out1.write(line)\n",
    "            else :\n",
    "                f_out2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_train_data(PATH_IN, PATH_OUT, FILE):\n",
    "    \n",
    "    with open(PATH_IN + 'answer.json', 'r') as f1, \\\n",
    "        open(PATH_IN + 'question_train.json', 'r') as f2, \\\n",
    "        open(PATH_IN + 'user.json', 'r') as f3, \\\n",
    "        open(PATH_IN + 'qtags.json', 'r') as f4 :\n",
    "    \n",
    "            answer = [json.loads(line.strip().strip(',')) for line in f1]\n",
    "            #print answer[0]\n",
    "            #answer_frame \n",
    "            answer_frame = DataFrame(answer)\n",
    "            #answer_frame = answer_frame.drop_duplicates('answer_content')\n",
    "            answer_frame = answer_frame.drop_duplicates('answer_id')\n",
    "            #print answer_frame #14464 x 5\n",
    "\n",
    "            question = [json.loads(line.strip().strip(',')) for line in f2]\n",
    "            #print question[0]\n",
    "            question_frame = DataFrame(question)\n",
    "            question_frame = question_frame.drop_duplicates('question_id')\n",
    "            #print question_frame  # 898 x 2\n",
    "\n",
    "            user = [json.loads(line.strip().strip(',')) for line in f3]\n",
    "            user_frame = DataFrame(user)\n",
    "            user_frame = user_frame.drop_duplicates('user_id')\n",
    "            #print user_frame    # 3378 x 14\n",
    "\n",
    "            qtag = [json.loads(line.strip().strip(',')) for line in f4]\n",
    "            qtag_frame = DataFrame(qtag)\n",
    "            qtag_frame = qtag_frame.drop_duplicates('question_id')\n",
    "            #print qtag_frame      # 9999 x 2\n",
    "\n",
    "            # Generate the source data\n",
    "            m1 = pd.merge(answer_frame, user_frame, how = 'outer')\n",
    "            m1 = m1.rename(columns = {'related_id':'question_id'})\n",
    "            #print m1.head()    # 14781 x 18\n",
    "\n",
    "            m2 = pd.merge(qtag_frame, question_frame)\n",
    "            #print m2.head()     # 898 x 3 \n",
    "\n",
    "            # Generate \n",
    "            m = pd.merge(m1, m2, on = 'question_id')\n",
    "            #print m\n",
    "            m = m.sort(['question_id','answer_voted'], ascending=[1,0])\n",
    "            m = m.drop('user_address', axis=1)\n",
    "            m = m.drop('user_name', axis=1)\n",
    "            m = m.reset_index(drop=True)\n",
    "            #print m\n",
    "\n",
    "            # modify the string in dataframe m\n",
    "            # m['answer_content'] = str_strip(m['answer_content'])\n",
    "            # m['question_content'] = str_strip(m['question_content'])\n",
    "            # m['user_edu'] = str_strip(m['user_edu'])\n",
    "            # m['user_specialty'] = str_strip(m['user_specialty'])\n",
    "            # m['user_interest'] = str_strip(m['user_interest'].astype(str))\n",
    "            # m['user_intro'] = str_strip(m['user_intro'])\n",
    "            # m['question_tags'] = str_strip(m['question_tags'].astype(str))\n",
    "            # m['user_recommends'] = str_strip(m['user_recommends'].astype(str))\n",
    "            \n",
    "            # print m      # 3195 x 20\n",
    "            # m.to_json(PATH_OUT + 'total01.json')\n",
    "\n",
    "            m.to_csv(PATH_OUT + FILE + '_Train.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_test_data(PATH_IN, PATH_OUT, FILE):\n",
    "    \n",
    "    with open(PATH_IN + 'answer.json', 'r') as f1, \\\n",
    "        open(PATH_IN + 'question_test.json', 'r') as f2, \\\n",
    "        open(PATH_IN + 'user.json', 'r') as f3, \\\n",
    "        open(PATH_IN + 'qtags.json', 'r') as f4 :\n",
    "    \n",
    "            answer = [json.loads(line.strip().strip(',')) for line in f1]\n",
    "            #print answer[0]\n",
    "            #answer_frame \n",
    "            answer_frame = DataFrame(answer)\n",
    "            answer_frame = answer_frame.drop_duplicates('answer_id')\n",
    "            #print answer_frame #14464 x 5\n",
    "\n",
    "            question = [json.loads(line.strip().strip(',')) for line in f2]\n",
    "            #print question[0]\n",
    "            question_frame = DataFrame(question)\n",
    "            question_frame = question_frame.drop_duplicates('question_id')\n",
    "            #print question_frame  # 898 x 2\n",
    "\n",
    "            user = [json.loads(line.strip().strip(',')) for line in f3]\n",
    "            user_frame = DataFrame(user)\n",
    "            user_frame = user_frame.drop_duplicates('user_id')\n",
    "            #print user_frame    # 3378 x 14\n",
    "\n",
    "            qtag = [json.loads(line.strip().strip(',')) for line in f4]\n",
    "            qtag_frame = DataFrame(qtag)\n",
    "            qtag_frame = qtag_frame.drop_duplicates('question_id')\n",
    "            #print qtag_frame      # 9999 x 2\n",
    "\n",
    "            # Generate the source data\n",
    "            m1 = pd.merge(answer_frame, user_frame, how = 'outer')\n",
    "            m1 = m1.rename(columns = {'related_id':'question_id'})\n",
    "            #print m1.head()    # 14781 x 18\n",
    "\n",
    "            m2 = pd.merge(qtag_frame, question_frame)\n",
    "            #print m2.head()     # 898 x 3 \n",
    "\n",
    "            # Generate \n",
    "            m = pd.merge(m1, m2, on = 'question_id')\n",
    "            #print m\n",
    "            m = m.sort(['question_id','answer_voted'], ascending=[1,0])\n",
    "            m = m.drop('user_address', axis=1)\n",
    "            m = m.drop('user_name', axis=1)\n",
    "            m = m.reset_index(drop=True)\n",
    "            #print m\n",
    "\n",
    "            # modify the string in dataframe m\n",
    "            # m['answer_content'] = str_strip(m['answer_content'])\n",
    "            # m['question_content'] = str_strip(m['question_content'])\n",
    "            # m['user_edu'] = str_strip(m['user_edu'])\n",
    "            # m['user_specialty'] = str_strip(m['user_specialty'])\n",
    "            # m['user_interest'] = str_strip(m['user_interest'].astype(str))\n",
    "            # m['user_intro'] = str_strip(m['user_intro'])\n",
    "            # m['question_tags'] = str_strip(m['question_tags'].astype(str))\n",
    "            # m['user_recommends'] = str_strip(m['user_recommends'].astype(str))\n",
    "            \n",
    "            # print m      # 3195 x 20\n",
    "            # m.to_json(PATH_OUT + 'total01.json')\n",
    "\n",
    "            m.to_csv(PATH_OUT + FILE + '_Test.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Data_Preparing(PATH_IN, PATH_OUT, FILE) :\n",
    "    \n",
    "    # RUN the function in pipline\n",
    "    generate_qapair(PATH_IN, PATH_OUT)\n",
    "    extract_qid(PATH_IN, PATH_OUT)\n",
    "    split_qustion(PATH_IN, PATH_OUT)\n",
    "    format_train_data(PATH_IN, PATH_OUT, FILE)\n",
    "    format_test_data(PATH_IN, PATH_OUT, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpgao/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:40: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/home/zpgao/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:39: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# FILE = 'Allergy'\n",
    "# FILE = 'Depression'\n",
    "# FILE = 'Alzheimer'\n",
    "# FILE = 'Arthritis'\n",
    "# PATH_IN  = './' + FILE + '/'\n",
    "# PATH_OUT = './' + FILE + '/'\n",
    "\n",
    "# Data_Preparing(PATH_IN, PATH_OUT, FILE)\n",
    "\n",
    "FILE_LIST = [\n",
    "    'Allergy',\n",
    "    'Depression',\n",
    "    'Alzheimer',\n",
    "    'Arthritis'\n",
    "]\n",
    "\n",
    "for FILE in FILE_LIST :\n",
    "    PATH_IN  = './' + FILE + '/'\n",
    "    PATH_OUT = './' + FILE + '/'\n",
    "    # print PATH_IN\n",
    "    Data_Preparing(PATH_IN, PATH_OUT, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# File_Type = 'Train'\n",
    "File_Type = 'Test'\n",
    "\n",
    "\n",
    "file_list = [   './Allergy/Allergy_' + File_Type +'.csv',\n",
    "                './Depression/Depression_' + File_Type + '.csv',\n",
    "                './Arthritis/Arthritis_' + File_Type +'.csv',\n",
    "                './Alzheimer/Alzheimer_' + File_Type +'.csv' ]\n",
    "\n",
    "# print train_files\n",
    "\n",
    "def load_data(file_list):\n",
    "    with open('./Total_' + File_Type + '.csv','w') as outfile:\n",
    "        for fname in file_list:\n",
    "            with open(fname) as infile:\n",
    "                outfile.write(infile.read())   \n",
    "\n",
    "load_data(file_list)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO THE TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop duplicates Q-A pair again\n",
    "\n",
    "Total_Train = pd.read_csv('./Total_Train.csv')     # 7804 rows × 18 columns\n",
    "Total_Test = pd.read_csv('./Total_Test.csv')       # 50293 rows × 18 columns\n",
    "\n",
    "Total_Train = Total_Train.drop_duplicates('answer_id')    # 6904 rows × 18 columns\n",
    "Total_Test = Total_Test.drop_duplicates('answer_id')      # 42513 rows × 18 columns\n",
    "\n",
    "Total_Train.to_csv('./Total_Train.csv', encoding='utf-8', index = False)\n",
    "Total_Test.to_csv('./Total_Test.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zpgao/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,2,3,4,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Total_Train.head()\n",
    "# Total_Train = Total_Train.str.replace(r'[^\\x00-\\x7f]',' ')\n",
    "\n",
    "Total_Train = pd.read_csv('./Total_Train.csv')     # 7804 rows × 18 columns\n",
    "Total_Test = pd.read_csv('./Total_Test.csv')       # 50293 rows × 18 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Str_Strip(data) :\n",
    "    data = data.str.replace(r'[^\\x00-\\x7f]',' ')\n",
    "    data = data.str.replace('\\\\n',' ')\n",
    "    data = data.str.replace('\\\\r',' ')\n",
    "    data = data.str.replace(',',' ')\n",
    "    data = data.str.replace('.',' ')\n",
    "    data = data.str.replace('\"','')\n",
    "    data = data.str.replace('u\\'','\\'')\n",
    "    data = data.str.replace('?','')\n",
    "    data = data.str.replace('\"','')\n",
    "    data = data.str.replace('.','')\n",
    "    data = data.str.replace(',','')\n",
    "    data = data.str.replace('/',' ')\n",
    "    data = data.str.replace('(','')\n",
    "    data = data.str.replace(')','')\n",
    "    data = data.str.replace('[','')\n",
    "    data = data.str.replace(']','')\n",
    "    data = data.str.replace('&','')\n",
    "    data = data.str.replace('\\'','')\n",
    "    data = data.str.replace('!','')\n",
    "    data = data.str.replace('-','')\n",
    "    data = data.str.replace(';','')\n",
    "    data = data.str.lower()   \n",
    "    return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def Stemming( Text_DataSet ) :\n",
    "    for i in range( len(Text_DataSet) ):\n",
    "        stemmed = []\n",
    "        # print Text_DataSet[i]\n",
    "        word_list = Text_DataSet[i].split(' ')\n",
    "        filtered = [word for word in word_list if word not in stopwords.words('english')]\n",
    "        # print filtered\n",
    "        for word in filtered :\n",
    "            #stemmed.append(PorterStemmer().stem(word))\n",
    "            stemmed.append(SnowballStemmer(\"english\").stem(word))\n",
    "        #print stemmed\n",
    "        stemmed =  [x.encode('UTF8') for x in stemmed]\n",
    "        # print stemmed\n",
    "        after_stemmed = ' '.join(stemmed)\n",
    "        Text_DataSet[i] = after_stemmed\n",
    "\n",
    "    return Text_DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Text_Processing(DataSet, ColName) :\n",
    "    \n",
    "    Text_DataSet = Str_Strip( DataSet[ColName].astype(str) )\n",
    "    Text_DataSet = Stemming( Text_DataSet )\n",
    "    DataSet[ColName] = Text_DataSet \n",
    "    \n",
    "    return DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Text_Col_List = [\n",
    "    'answer_content',\n",
    "    'question_content',\n",
    "    'question_tags',\n",
    "    \n",
    "    'user_edu',\n",
    "    'user_specialty',\n",
    "    'user_interest',\n",
    "    'user_intro',\n",
    "    'user_recommends'\n",
    "]\n",
    "\n",
    "for ColName in Text_Col_List :\n",
    "    \n",
    "    Total_Train = Text_Processing(Total_Train, ColName)\n",
    "\n",
    "for ColName in Text_Col_List :\n",
    "    \n",
    "    Total_Test = Text_Processing(Total_Test, ColName)\n",
    "\n",
    "#Total_Train = Text_Processing(Total_Train)\n",
    "#Total_Test = Text_Processing(Total_Test)\n",
    "\n",
    "#Total_Train.to_csv('./Total_Train.csv', encoding='utf-8', index = False)\n",
    "#Total_Test.to_csv('./Total_Test.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Total_Train.to_csv('./Total_Train_AfterProcessing.csv', encoding='utf-8', index = False)\n",
    "Total_Test.to_csv('./Total_Test_AfterProcessing.csv', encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
